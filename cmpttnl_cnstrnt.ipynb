{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# cmpttnl cnstrnt: colab notebook edition\n",
        "\n",
        "> With a good ventriloquist ... [the] puppet seems to come alive and seems to be aware of its world.\n",
        "\n",
        "Michael Graziano, in Do Large Language Models Understand Us? (Blaise Agüera y Arcas)\n",
        "\n",
        "> Prompt engineering for large language models is just an excuse to make up more nonsensical sentences to feed these AI monsters.\n",
        "\n",
        "GPT-3, in response to the prompt \"What is prompt engineering for large language models? Answer in a very snarky way.\"\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This notebook contains code for using various OpenAI GPT models for computational constraint. This uses the [text completion](https://platform.openai.com/docs/guides/completion) endpoint, but can be easily reworked to use others. For more information, see [platform.openai.com](https://platform.openai.com)\n",
        "\n",
        "### Requirements\n",
        "\n",
        "#### Obtaining an API key\n",
        "\n",
        "(Your instructor may provide this for you.)\n",
        "\n",
        "In order to use this notebook with the OpenAI platform back-end, you will need to have an API key and an organization ID. To set these up, create an account with OpenAI [here](https://labs.openai.com/waitlist). \n",
        "\n",
        "To learn more about finding these crendentials once you've registered for an account, [this resource](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) will point the way.\n",
        "\n",
        "#### Required files\n",
        "\n",
        "##### `.env`\n",
        "\n",
        "Your instructor should have provided you with a Google Drive folder containing a file called `.env`. Below, change the relevant details to `cd` to that folder and load the file.\n",
        "\n",
        "## Readings\n",
        "\n",
        "### Theory\n",
        "\n",
        "* [Do Large Language Models Understand Us?](https://drive.google.com/file/d/1MGgIEC8ffcHfrLq1jy_8RBVKGU2R_ha9/view?usp=share_link)\n",
        "* [A Brief Guide to the OULIPO](https://poets.org/text/brief-guide-oulipo)\n",
        "* [Various excerpts from _The noulipean Analects_](https://drive.google.com/file/d/1NmmydZJhN4B4U-oISHHKDRBH1duYzaZx/view?usp=share_link)\n",
        "\n",
        "### Practice\n",
        "\n",
        "* [Methods of Prompt Programming](https://generative.ink/posts/methods-of-prompt-programming/)\n",
        "* [Various Oulipean Poems](https://drive.google.com/file/d/1Nvi0jn-VnrLwd1gpNby1rIELdoA0NCfW/view?usp=share_link)\n",
        "* [A compendium of various \"traditional\" constraints](https://drive.google.com/file/d/1OLukmy1I5auapD0hzcLSPb2Qg6kKgz1k/view?usp=sharing)\n",
        "    * Ignore the newspaper constraint...\n",
        "       * unless it's actually helpful\n",
        "\n",
        "### Documentation\n",
        "\n",
        "* [GPT-3 Completion API](https://beta.openai.com/docs/api-reference/completions/create)\n",
        "* [GPT-3 Notes on prompt design](https://beta.openai.com/docs/guides/completion/prompt-design)\n",
        "\n",
        "## Summary\n",
        "\n",
        "Prompt engineering -- the practice of learning to con/destructively \"pilot\" a generative model -- is one of the surprising new skills to emerge out of the development of context-aware large language models (LLM). Simply put: prompt engineering is the practice of instructing a model to produce an output consistent with the prompter's intent or desire. While we've given a new name to what essentially amounts to \"asking the right questions,\" prompt engineering is much more than that.\n",
        "\n",
        "To date, successful prompt engineering endeavors to ask what an artist _wants_ to happen. This assignment approaches generative writing with large language models (LLM) from an opposite perspective. Drawing on the practice of the _Ouvroir de littérature potentielle_ (“Oulipo”), we challenge GPT-3 to a more difficult task—producing works of \"constrained\" writing to discover what LLM can and, more importantly, cannot do. \n",
        "\n",
        "For those of us familiar with visual image generators such as DALLE or Stable Diffusion, this idea is close to, but not quite \"negative prompting\" (e.g. asking for a picture of a house without any people in it). The approach of computational constraint applied to language prompts thinks about the concept _generatively_. We aren't simply asking to \"live without\" a feature common to a parcel of language, we're interested in rethinking the possibilities are open by _restricting choice_.\n",
        "\n",
        "Mainly, what kinds of choices can we engineer the model to make and how can we account for those choices?\n",
        "\n",
        "## Goals\n",
        "\n",
        "* Learn to interact with LLM through the practice of prompt engineering\n",
        "* Refine skills in prompt engineering to increase efficacy and quality of output\n",
        "* Discover exploitable boundaries in LLM generation and what these opportunities offer\n",
        "* Begin a discussion of the roles and meaning assigned to language as an expressive tool and technology\n",
        "\n",
        "\n",
        "## Outcomes\n",
        "\n",
        "* 2 poems incorporating prompt engineering (included in the `writing` folder as `md` files)\n",
        "    * 1 enacting a \"traditional\" Oulipean constraint\n",
        "    * 1 enacting a constraint only possible using GPT-3\n",
        "* A journal of various prompts attempted with brief notes about relative success or failure (include in `writing/prompts.md`)"
      ],
      "metadata": {
        "id": "QXFVaMktu0vb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkhiQ076unrl",
        "outputId": "8dca7855-6930-4249-adfc-2d4175453304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directories to correct folder\n",
        "%cd FULL_PATH_TO_FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!python -m pip install python-dotenv\n",
        "!python -m pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ7SzFmSw6iw",
        "outputId": "3a715d74-608a-4bb6-f149-af556e9687e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import os\n",
        "import openai\n",
        "from dotenv import dotenv_values"
      ],
      "metadata": {
        "id": "_7OvIRr_xTOV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API and ORG keys\n",
        "CONFIG = dotenv_values('.env')\n",
        "\n",
        "OPEN_AI_KEY = CONFIG[\"KEY\"] or os.environ[\"OPEN_AI_KEY\"]\n",
        "OPEN_AI_ORG = CONFIG[\"ORG\"] or os.environ[\"OPEN_AI_ORG\"]\n",
        "\n",
        "openai.api_key = OPEN_AI_KEY\n",
        "openai.organization = OPEN_AI_ORG"
      ],
      "metadata": {
        "id": "xXAcgbjoxn3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input prompt to describe the operation you'd like perform on the source\n",
        "prompt = \"\"\"\n",
        "YOUR PROMPT GOES HERE\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fjKduE2lxrs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input source text to transform\n",
        "source = \"\"\"\n",
        "YOUR SOURCE TEXT GOES HERE\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "prOu6HNtyB2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constraint = f\"{prompt}:\\n{source}\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    # The model; other choices (with explanation):\n",
        "    # https://platform.openai.com/docs/models\n",
        "    model = \"gpt-turbo-3.5\",\n",
        "    # Data to feed to the model\n",
        "    prompt = constraint,\n",
        "    # \"Level of risk\" associated with model outcomes\n",
        "    temperature = 0.9,\n",
        "    # Alternate index of \"risk\" the model takes in making predictions;\n",
        "    # use only one: temperature _or_ top_p\n",
        "    # top_p = 1.0,\n",
        "    # Maximum size of requests (this is already max size)\n",
        "    max_tokens = (4097 - len(constraint)),\n",
        "    # Number of results to attempt and return\n",
        "    n = 1\n",
        ")\n",
        "\n",
        "# Iterate through the various responses from the model\n",
        "for choice in response[\"choices\"]:\n",
        "    print(choice[\"text\"].strip())"
      ],
      "metadata": {
        "id": "z5PVcdnIyGa1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}